# -*- coding: utf-8 -*-
"""House_Pricing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1h48E6moLRaJIZgspKtG3TJBdv2TWmur1
"""

from google.colab import files
uploaded=files.upload()

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

train=pd.read_csv('train.csv')
test=pd.read_csv('test.csv')

train.head(10)

(train.shape)

test.shape

##gives types of features of dataset
train.info()

## gives stats like mean , count etc.
train.describe()

## check the missing values
train.isnull().sum()

## dropping the unecessary features
X=train.drop(['Id','SalePrice'],axis=1)
## X(features) ->things we use to predict.....Since id has no link with pricing , model thinks higher the id higher the price,
## so we remove the id ....Saleprice is our target we have to remove it as well

y=train['SalePrice'] ## Y(target) things we want to predict...our target so we keep it

X.head(10)

y.head(2)

## removing the missing values
X.fillna(0)
X.isnull().sum()

X=pd.get_dummies(X) ## it converts all datatypes into numbers
X.info() ## to verify this step

test=pd.get_dummies(test)
test.info()

## train/test
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)

## importing model libraries
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error, r2_score

##models used
models = {
    'Linear Regression': LinearRegression(),
    'Decision Tree': DecisionTreeRegressor(),
    'K-Nearest Neighbors': KNeighborsRegressor()
}

## result dictionary
results = {}
for model_name, model in models.items():
    model.fit(X_train, y_train) ## train
    y_pred = model.predict(X_test) ## prediction of my model
    rmse = np.sqrt(mean_squared_error(y_test, y_pred)) ## calculates root mean squared error
    r2 = r2_score(y_test, y_pred) ## calculates r2 score
    results[model_name] = {'RMSE': rmse, 'R-squared': r2}
    pd.DataFrame(results).T
    print( results[model_name])

## best model
best_model = DecisionTreeRegressor(max_depth=5, random_state=42)
best_model.fit(X, y)

## user interactive

import pandas as pd

def predict_price_interactive():
    # Ask user for input
    gr_liv_area = float(input("Enter Living Area (sq ft): "))
    overall_qual = int(input("Enter Overall Quality (1-10): "))
    garage_cars = int(input("Enter Number of Garage Cars: "))
    full_bath = int(input("Enter Number of Full Baths: "))
    year_built = int(input("Enter Year Built: "))

    # Create a sample dictionary
    sample = {
        "GrLivArea": gr_liv_area,
        "OverallQual": overall_qual,
        "GarageCars": garage_cars,
        "FullBath": full_bath,
        "YearBuilt": year_built
    }

    # Convert to DataFrame
    sample_df = pd.DataFrame([sample])

    # Align columns with training features
    sample_df = sample_df.reindex(columns=X.columns, fill_value=0)

    # Predict price
    price = best_model.predict(sample_df)[0]

    # Display result
    print(f"\n Predicted House Price: ${price:,.0f}")

# Call the function
predict_price_interactive()